{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 09. Exercise 03\n",
    "# Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier,VotingClassifier,BaggingClassifier,StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create the same dataframe as in the previous exercise.\n",
    "2. Using `train_test_split` with parameters `test_size=0.2`, `random_state=21` get `X_train`, `y_train`, `X_test`, `y_test` and then get `X_train`, `y_train`, `X_valid`, `y_valid` from the previous `X_train`, `y_train`. Use the additional parameter `stratify`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = pd.read_csv('../data/day-of-week-not-scaled.csv')\n",
    "df_y = pd.read_csv('../data/dayofweek.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_x\n",
    "y = df_y['dayofweek']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full,y_test = train_test_split(X,y,test_size=0.2,random_state=21,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=21, stratify=y_train_full\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Individual classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train SVM, decision tree and random forest again with the best parameters that you got from the 01 exercise with `random_state=21` for all of them.\n",
    "2. Evaluate `accuracy`, `precision`, and `recall` for them on the validation set.\n",
    "3. The result of each cell of the section should look like this:\n",
    "\n",
    "```\n",
    "accuracy is 0.87778\n",
    "precision is 0.88162\n",
    "recall is 0.87778\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.87778\n",
      "precision is 0.88461\n",
      "recall is 0.87778\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(C=10,class_weight=None,gamma='auto',probability=True,kernel='rbf', random_state=21).fit(X_train,y_train)\n",
    "y_pred = svc.predict(X_valid)\n",
    "accuracy = accuracy_score(y_pred,y_valid)\n",
    "precision = precision_score(y_pred,y_valid,average='weighted')\n",
    "recall = recall_score(y_pred,y_valid,average='weighted')\n",
    "print(f'accuracy is {accuracy:.5f}')\n",
    "print(f'precision is {precision:.5f}')\n",
    "print(f'recall is {recall:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.86667\n",
      "precision is 0.86892\n",
      "recall is 0.86667\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(class_weight='balanced',criterion='gini',max_depth=22,random_state=21).fit(X_train,y_train)\n",
    "y_pred = dt.predict(X_valid)\n",
    "accuracy = accuracy_score(y_pred,y_valid)\n",
    "precision = precision_score(y_pred,y_valid,average='weighted')\n",
    "recall = recall_score(y_pred,y_valid,average='weighted')\n",
    "print(f'accuracy is {accuracy:.5f}')\n",
    "print(f'precision is {precision:.5f}')\n",
    "print(f'recall is {recall:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.88519\n",
      "precision is 0.88875\n",
      "recall is 0.88519\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100,max_depth=24,class_weight='balanced',criterion='entropy',random_state=21).fit(X_train,y_train)\n",
    "y_pred = rf.predict(X_valid)\n",
    "accuracy = accuracy_score(y_pred,y_valid)\n",
    "precision = precision_score(y_pred,y_valid,average='weighted')\n",
    "recall = recall_score(y_pred,y_valid,average='weighted')\n",
    "print(f'accuracy is {accuracy:.5f}')\n",
    "print(f'precision is {precision:.5f}')\n",
    "print(f'recall is {recall:.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Voting classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using `VotingClassifier` and the three models that you have just trained, calculate the `accuracy`, `precision`, and `recall` on the validation set.\n",
    "2. Play with the other parameteres.\n",
    "3. Calculate the `accuracy`, `precision` and `recall` on the test set for the model with the best weights in terms of accuracy (if there are several of them with equal values, choose the one with the higher precision)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A voting classifier is a machine learning model that gains experience by training on a collection of several models and forecasts an output (class) based on the class with the highest likelihood of becoming the output. To forecast the output class based on the largest majority of votes, it averages the results of each classifier provided into the voting classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard Voting: In hard voting, the predicted output class is a class with the highest majority of votes, i.e., the class with the highest probability of being predicted by each classifier. For example, let's say classifiers predicted the output classes as (Cat, Dog, Dog). As the classifiers predicted class \"dog\" a maximum number of times, we will proceed with Dog as our final prediction.\n",
    "\n",
    "Soft Voting: In this, the average probabilities of the classes determine which one will be the final prediction. For example, let's say the probabilities of the class being a \"dog\" is (0.30, 0.47, 0.53) and a \"cat\" is (0.20, 0.32, 0.40). So, the average for a class dog is 0.4333, and the cat is 0.3067, from this, we can confirm our final prediction to be a dog as it has the highest average probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.90000\n",
      "precsion is 0.90185\n",
      "recall is 0.90000\n"
     ]
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "    estimators=\n",
    "    [('svc',svc),('dt',dt),('rf',rf)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train,y_train)\n",
    "y_pred = voting_clf.predict(X_valid)\n",
    "accuracy = accuracy_score(y_pred,y_valid)\n",
    "precision = precision_score(y_pred,y_valid,average='weighted')\n",
    "recall = recall_score(y_pred,y_valid,average='weighted')\n",
    "print(f'accuracy is {accuracy:.5f}')\n",
    "print(f'precsion is {precision:.5f}')\n",
    "print(f'recall is {recall:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.88519\n",
      "precsion is 0.88599\n",
      "recall is 0.88519\n"
     ]
    }
   ],
   "source": [
    "voting_clf_soft =VotingClassifier(\n",
    "    estimators=\n",
    "    [('svc',svc),('dt',dt),('rf',rf)],\n",
    "    voting='soft'\n",
    ")\n",
    "voting_clf_soft.fit(X_train,y_train)\n",
    "y_pred = voting_clf_soft.predict(X_valid)\n",
    "accuracy = accuracy_score(y_pred,y_valid)\n",
    "precision = precision_score(y_pred,y_valid,average='weighted')\n",
    "recall = recall_score(y_pred,y_valid,average='weighted')\n",
    "print(f'accuracy is {accuracy:.5f}')\n",
    "print(f'precsion is {precision:.5f}')\n",
    "print(f'recall is {recall:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Voting Classifier\n",
      "best param used is {'voting': 'soft', 'weights': (4, 1, 4)}\n",
      "accuracy is 0.90741\n",
      "precision is 0.91275\n",
      "recall is 0.90741\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_score = 0\n",
    "best_metrics = {}\n",
    "best_model = None\n",
    "best_weight = None\n",
    "best_param = {}\n",
    "\n",
    "weight_options = [\n",
    "    (1, 1, 1),\n",
    "    (2, 1, 1), (1, 2, 1), (1, 1, 2),\n",
    "    (2, 2, 1), (2, 1, 2), (1, 2, 2),\n",
    "    (3, 1, 1), (1, 3, 1), (1, 1, 3),\n",
    "    (4, 1, 1), (1, 4, 1),\n",
    "    (4, 1, 4), (4, 4, 1), (1, 4, 4)\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "voting_options = ['hard', 'soft']\n",
    "\n",
    "for weight in weight_options:\n",
    "    for voting in voting_options:\n",
    "        model = VotingClassifier(\n",
    "            estimators=\n",
    "            [('svc',svc),('dt',dt),('rf',rf)],\n",
    "            voting= voting,\n",
    "            weights=weight\n",
    "        )\n",
    "        model.fit(X_train,y_train)\n",
    "        y_pred = model.predict(X_valid)\n",
    "        accuracy = accuracy_score(y_pred,y_valid)\n",
    "        precision = precision_score(y_pred,y_valid,average='weighted')\n",
    "        recall = recall_score(y_pred,y_valid,average='weighted')\n",
    "\n",
    "        if accuracy > best_score or (accuracy == best_score and precision > best_metrics.get('precision',0)):\n",
    "            best_score = accuracy\n",
    "            best_model = model\n",
    "            best_param = {'voting':voting,'weights':weight}\n",
    "            best_metrics = {'accuracy':accuracy,'precision':precision,'recall':recall}\n",
    "\n",
    "\n",
    "\n",
    "print('Best Voting Classifier')\n",
    "print(f'best param used is {best_param}')\n",
    "print(f'accuracy is {best_metrics['accuracy']:.5f}')\n",
    "print(f'precision is {best_metrics['precision']:.5f}')\n",
    "print(f'recall is {best_metrics['recall']:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bagging classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using `BaggingClassifier` and `SVM` with the best parameters create an ensemble, try different values of the `n_estimators`, use `random_state=21`.\n",
    "2. Play with the other parameters.\n",
    "3. Calculate the `accuracy`, `precision`, and `recall` for the model with the best parameters (in terms of accuracy) on the test set (if there are several of them with equal values, choose the one with the higher precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging Classifier\n",
    "Bagging or Bootstrap aggregating is a type of ensemble learning in which multiple base models are trained independently and parallelly on different subsets of training data. Each subset is generated using bootstrap sampling in which data points are picked at randomly with replacement. In bagging classifier the final prediction is made by aggregating the predictions of all base model using majority voting. In the models of regression the final prediction is made by averaging the predictions of the all base model and that is known as bagging regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.88148\n",
      "precsion is 0.89042\n",
      "recall is 0.88148\n"
     ]
    }
   ],
   "source": [
    "bagging_clf = BaggingClassifier(svc,n_estimators=50,\n",
    "                                random_state=21)\n",
    "\n",
    "\n",
    "bagging_clf.fit(X_train,y_train)\n",
    "y_pred = bagging_clf.predict(X_valid)\n",
    "accuracy = accuracy_score(y_pred,y_valid)\n",
    "precision = precision_score(y_pred,y_valid,average='weighted')\n",
    "recall = recall_score(y_pred,y_valid,average='weighted')\n",
    "print(f'accuracy is {accuracy:.5f}')\n",
    "print(f'precsion is {precision:.5f}')\n",
    "print(f'recall is {recall:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Bagging classifier\n",
      "best params used is {'n_estimators': 50, 'max_samples': 1.0, 'max_features': 1.0}\n",
      "accuracy is 0.908284023668639\n",
      "precision is 0.9130698211142415\n",
      "recall is 0.908284023668639\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_score = 0\n",
    "best_metrics = {}\n",
    "best_model = None\n",
    "best_param = None\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators':50,'max_samples':1.0,'max_features':1.0},\n",
    "    {'n_estimators':100,'max_samples':0.8,'max_features':0.8},\n",
    "    {'n_estimators':150,'max_samples':0.9,'max_features':1.0},\n",
    "    {'n_estimators':150,'max_samples':1.0,'max_features':1.0},\n",
    "    ]\n",
    "\n",
    "for param in param_grid:\n",
    "    model = BaggingClassifier(svc,\n",
    "                              n_estimators=param['n_estimators'],\n",
    "                              max_samples=param['max_samples'],\n",
    "                              max_features=param['max_features'],\n",
    "                              random_state=21,\n",
    "                              n_jobs=8)\n",
    "    \n",
    "    model.fit(X_train_full,y_train_full)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_pred,y_test)\n",
    "    precision = precision_score(y_pred,y_test,average='weighted')\n",
    "    recall = recall_score(y_pred,y_test,average='weighted')\n",
    "\n",
    "    if accuracy > best_score or (accuracy == best_score and precision > best_metrics.get('precision',0)):\n",
    "        best_score = accuracy\n",
    "        best_model = model\n",
    "        best_param = param\n",
    "        best_metrics = {'accuracy':accuracy,\n",
    "                        'precision':precision,\n",
    "                        'recall':recall}\n",
    "        \n",
    "print('Best Bagging classifier')\n",
    "print(f'best params used is {best_param}')\n",
    "print(f'accuracy is {best_metrics['accuracy']}')\n",
    "print(f'precision is {best_metrics['precision']}')\n",
    "print(f'recall is {best_metrics['recall']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stacking classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To achieve reproducibility in this case you will have to create an object of cross-validation generator: `StratifiedKFold(n_splits=n, shuffle=True, random_state=21)`, where `n` you will try to optimize (the details are below).\n",
    "2. Using `StackingClassifier` and the three models that you have recently trained, calculate the `accuracy`, `precision` and `recall` on the validation set, try different values of `n_splits` `[2, 3, 4, 5, 6, 7]` in the cross-validation generator and parameter `passthrough` in the classifier itself,\n",
    "3. Calculate the `accuracy`, `precision`, and `recall` for the model with the best parameters (in terms of accuracy) on the test set (if there are several of them with equal values, choose the one with the higher precision). Use `final_estimator=LogisticRegression(solver='liblinear')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking is a ensemble learning technique where the final model known as the “stacked model\" combines the predictions from multiple base models. The goal is to create a stronger model by using different models and combining them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Stacking Classifier\n",
      "Best params used is {'n_splits': 4, 'passthrough': False}\n",
      "accuracy is 0.9111111111111111\n",
      "precision is 0.9148447718623157\n",
      "recall is 0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "estimators= [('svc',svc),('dt',dt),('rf',rf)]\n",
    "\n",
    "best_score = 0\n",
    "\n",
    "results = []\n",
    "best_metrics = {}\n",
    "best_param = {}\n",
    "final_estimator = LogisticRegression(solver='liblinear')\n",
    "\n",
    "for n in [2,3,4,5,6,7]:\n",
    "\n",
    "# for n in [4]:\n",
    "    cv = StratifiedKFold(n_splits=n,random_state=21,shuffle=True)\n",
    "    for passthrough in [True,False]:\n",
    "        stacking_clf = StackingClassifier(\n",
    "            estimators=estimators,\n",
    "            cv=cv,\n",
    "            final_estimator=final_estimator,\n",
    "            passthrough=passthrough,\n",
    "            n_jobs=8\n",
    "        )\n",
    "\n",
    "        stacking_clf.fit(X_train,y_train)\n",
    "        y_pred = stacking_clf.predict(X_valid)\n",
    "        accuracy = accuracy_score(y_pred,y_valid)\n",
    "        precision = precision_score(y_pred,y_valid,average='weighted')\n",
    "        recall = recall_score(y_pred,y_valid,average='weighted')\n",
    "\n",
    "        if accuracy > best_score or (accuracy == best_score and precision > best_metrics.get('precision',0)):\n",
    "            best_score = accuracy\n",
    "\n",
    "            best_param = {'n_splits':n, 'passthrough':passthrough}\n",
    "            best_metrics = {'accuracy':accuracy,'precision':precision,'recall':recall}\n",
    "\n",
    "print(\"Best Stacking Classifier\")\n",
    "print(f'Best params used is {best_param}')\n",
    "print(f'accuracy is {best_metrics['accuracy']}')\n",
    "print(f'precision is {best_metrics['precision']}')\n",
    "print(f'recall is {best_metrics['recall']}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose the best model in terms of accuracy (if there are several of them with equal values, choose the one with the higher precision).\n",
    "2. Analyze: for which weekday your model makes the most errors (in % of the total number of samples of that class in your full dataset), for which labname and for which users.\n",
    "3. Save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier\n",
      "accuracy is 0.88462\n",
      "precsion is 0.89252\n",
      "recall is 0.88462\n"
     ]
    }
   ],
   "source": [
    "bagging_clf = BaggingClassifier(svc,n_estimators=50,\n",
    "                                random_state=21)\n",
    "\n",
    "\n",
    "bagging_clf.fit(X_train,y_train)\n",
    "y_pred = bagging_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_pred,y_test)\n",
    "precision = precision_score(y_pred,y_test,average='weighted')\n",
    "recall = recall_score(y_pred,y_test,average='weighted')\n",
    "print(\"Bagging Classifier\")\n",
    "print(f'accuracy is {accuracy:.5f}')\n",
    "print(f'precsion is {precision:.5f}')\n",
    "print(f'recall is {recall:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier\n",
      "accuracy is 0.90828\n",
      "precsion is 0.91306\n",
      "recall is 0.90828\n"
     ]
    }
   ],
   "source": [
    "estimators= [('svc',svc),('dt',dt),('rf',rf)]\n",
    "\n",
    "final_estimator = LogisticRegression(solver='liblinear')\n",
    "\n",
    "n = 4\n",
    "\n",
    "cv = StratifiedKFold(n_splits=n,random_state=21,shuffle=True)\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    cv=cv,\n",
    "    final_estimator=final_estimator,\n",
    "    passthrough=True,\n",
    "    n_jobs=8\n",
    "    )\n",
    "\n",
    "stacking_clf.fit(X_train,y_train)\n",
    "y_pred = stacking_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_pred,y_test)\n",
    "precision = precision_score(y_pred,y_test,average='weighted')\n",
    "recall = recall_score(y_pred,y_test,average='weighted')\n",
    "\n",
    "     \n",
    "\n",
    "print(\"Stacking Classifier\")\n",
    "print(f'accuracy is {accuracy:.5f}')\n",
    "print(f'precsion is {precision:.5f}')\n",
    "print(f'recall is {recall:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model is Voting Classifier\n",
      "accuracy is 0.91124\n",
      "precsion is 0.91370\n",
      "recall is 0.91124\n"
     ]
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "    estimators=\n",
    "    [('svc',svc),('dt',dt),('rf',rf)],\n",
    "    voting='hard',\n",
    "    weights=(4,1,4))\n",
    "voting_clf.fit(X_train,y_train)\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_pred,y_test)\n",
    "precision = precision_score(y_pred,y_test,average='weighted')\n",
    "recall = recall_score(y_pred,y_test,average='weighted')\n",
    "\n",
    "print('Best model is Voting Classifier')\n",
    "print(f'accuracy is {accuracy:.5f}')\n",
    "print(f'precsion is {precision:.5f}')\n",
    "print(f'recall is {recall:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>338 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      true  pred\n",
       "1087     1     1\n",
       "16       5     5\n",
       "563      6     6\n",
       "1381     3     3\n",
       "1199     2     2\n",
       "...    ...   ...\n",
       "1411     3     3\n",
       "1079     1     1\n",
       "1222     2     2\n",
       "1064     1     1\n",
       "1243     2     2\n",
       "\n",
       "[338 rows x 2 columns]"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({'true': y_test, 'pred': y_pred})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true\n",
       "5    7\n",
       "0    7\n",
       "6    5\n",
       "1    3\n",
       "4    3\n",
       "2    3\n",
       "3    2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "weekday = {0: \"Monday\", 1: \"Tuesday\", 2: \"Wednesday\", 3: \"Thursday\",\n",
    "           4: \"Friday\", 5: \"Saturday\", 6: \"Sunday\"}\n",
    "\n",
    "\n",
    "errors = results[results['true'] != results['pred']]['true'].value_counts()\n",
    "totals = results['true'].value_counts()\n",
    "\n",
    "errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification percentages per weekday:\n",
      "Monday: 25.93%\n",
      "Friday: 14.29%\n",
      "Saturday: 12.96%\n",
      "Wednesday: 10.0%\n",
      "Sunday: 7.04%\n",
      "Tuesday: 5.45%\n",
      "Thursday: 2.5%\n",
      "\n",
      "Weekday with most errors: Monday (25.93%)\n"
     ]
    }
   ],
   "source": [
    "error_percent = (errors / totals * 100).round(2).fillna(0)\n",
    "\n",
    "print(\"Misclassification percentages per weekday:\")\n",
    "for day_num in error_percent.sort_values(ascending=False).index:\n",
    "    print(f\"{weekday[day_num]}: {error_percent[day_num]}%\")\n",
    "\n",
    "worst_day = error_percent.idxmax()\n",
    "print(f\"\\nWeekday with most errors: {weekday[worst_day]} ({error_percent[worst_day]}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_cols = [col for col in X_test.columns if col.startswith('labname_')]\n",
    "results['labname'] = X_test[lab_cols].idxmax(axis=1).str.replace('labname_', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lab with most errors: lab03 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "lab_errors = results[results['true'] != results['pred']]['labname'].value_counts()\n",
    "lab_totals = results['labname'].value_counts()\n",
    "lab_error_percent = (lab_errors / lab_totals * 100).round(2).fillna(0)\n",
    "worst_lab = lab_error_percent.idxmax()\n",
    "print(f\"Lab with most errors: {worst_lab} ({lab_error_percent[worst_lab]}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_cols = [col for col in X_test.columns if col.startswith('uid_user_')]\n",
    "results['user'] = X_test[user_cols].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User with most errors: uid_user_6 (50.0%)\n"
     ]
    }
   ],
   "source": [
    "user_errors = results[results['true'] != results['pred']]['user'].value_counts()\n",
    "user_totals = results['user'].value_counts()\n",
    "user_error_percent = (user_errors / user_totals * 100).round(2).fillna(0)\n",
    "worst_user = user_error_percent.idxmax()\n",
    "print(f\"User with most errors: {worst_user} ({user_error_percent[worst_user]}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/VotingClassifier_bestmodel.pkl']"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(best_model,\"../data/VotingClassifier_bestmodel.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
